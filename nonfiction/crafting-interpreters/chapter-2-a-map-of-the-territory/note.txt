04/12/2024

Language implementation
        Details such as:
                Stack
                Bytecode
                Recursive Descent

Language specification
        The behavior of the language itself in
        certain scenarios

2.1: The Parts of a Language
        Making a language mirrors what has been done since computing
        has been around

        A language starts at the high level(the language) and works
        its way down into the low level(translated into something a computer understands)

        2.1.1: Scanning
                Also known as lexing / lexical analysis

                Takes a linear stream of characters and chunks them together into
                a series of "words"

                Each word is called a token
                        Single characters
                                ( ,
                        Several characters
                                Numbers(123)
                                String literals("hi")
                                Identifiers(min)

                Some characters don't mean anything, and is discarded
                        Whitespace
                        Comments

        Ex: var average = ( min + max) / 2;

        2.1.2: Parsing
                Where syntax gets a grammar

                Grammar - the ability to compose larger expressions and statements out of
                smaller parts

                Takes the sequence of tokens anad builds a tree structure that mirros the nested
                natrue of the grammar

                Tree structure can be called:
                        Parse tree
                        Abstract syntax tree
                        Syntax tree
                        AST
                        Tree
                Ex: See image/abstract-syntax-tree.png

                Closely tied to the artifical intelligence community, where many
                techniques to parse programming languages were created to parse
                human languages in research

                Reports syntax errors when the something doesn't match the grammar
                of the language

        2.1.3: Static Analysis
                Scanning and parsing are similar across all programming language
                implementations. 

                At this point we know the syntactic structure of the code but not
                much else.
                        Ex: a + b
                        We know we are adding a and b, but what do a and b
                        refer to?

                Binding / Resolution
                        For each identifier, we find out where the name is defined
                        and wire the two together

                        Scope - the region of source code where a certain name can be used
                        to refer to a certain declaration

                If a language is statically typed, we do a type check. Once we know where a and
                b are declared, we can figure out their types. If the types don't support the
                + operation we throw a type error

                This extra information can be stored in a couple different places:
                        As attributes on the syntax tree. The attributes would
                        be null during the parsing phase

                        Symbol table - a lookup table where the keys are the identifiers
                        (names of variables and declarations)

                        Transform the tree into a new data structure that expresses the
                        semantics of the code more directly

        Each stage of a compiler is to organize the program info in a way that makes the
        next stage simpler to implement

        Front end - Specific to the source language the program is written in
        Back end - Specific to the final architecture where the program is run 

        2.1.4: Intermediate representations
                In the middle of the front and back end, code may be stored
                as an intermediate representation (IR) that isn't tightly tied to either
                the source or destination forms.

                Acts as an interface between these two languages

                Lets you support multiple source langauges and target platforms with less effort
                        Ex: Instread of writing full compilers for each aritecture, you can
                        write a front end for a language that produces an IR. Then one back
                        end for each architecture that uses the IR

        2.1.5: Optimization
                Once the semantics of a program is understand, we can optimize
                it by altering the implemtating while keeping the same outcome

                Ex: Constant folding
                        If some expression always evaluates to the exact same value,
                        we can do the evaluation at compile time and replace the
                        code for the expression with its result

                        pennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);

                        pennyArea = 0.4417860938

                Big part of programming language creation. Really important to
                get as much performance as possible out of the language

        2.1.6: Code Generation
                Last step, generating code(code gen) where "code" refers to
                the assembly-like instructions a CPU runs and not the kind of
                "source code" a human might want to read

                Part of back end, each step after this makes the code more and
                more primitive

                Should we generate instructions for a real CPU or a virtual one?
                        Native code 
                                Fast and can be loaded directly
                                on the chip, but is hard to generate.
                        
                                Tied to a particular architecture

                        Virtual code
                                Code for a hypothetical, idealized machine. Sometimes
                                called p-code(portable) or bytecode

                                Maps closer to the language semantics

                                Not tied to a particular architecture

        2.1.7: Virtual Machine
                If the compiler produces bytecode, this is the next step. Since no
                chip can run the bytecode, it must be translated.

                Two options:
                        Write a compiler for the bytecode for each target architecture that
                        converts it into native code

                        Write a virtual machine

                Virtual machine(VM) - a program that emulates a hypothetical chip supporting your
                virtual architecture at runtime.

                Slower than translating it to native code ahead of time because every instruction
                must be simulated at runtime each time it executes.

                Simplier and more portable

                Ex: Implement a VM in C and run the language in any architecture C supports

        2.1.8: Runtime
                Native - load the executable

                VM - start up and load bytecode

                Provides services such as memory management, "instance of" tests(object types)

                Compiled language - inserted into executable

                Interpreter / VM - lives there

2.2: Shortcuts and Alternate Routes
        2.2.1: Single-Pass Compilers
                Interleave parsing, analysis, and code generation so that
                they produce output code directly into the parser, without ever
                allocating any syntax trees or IRs

                Restrict the disgn of the language

                Don't store global info and don't revist and previously parsed
                part of the code. As soon as an expression is seen, enough needs
                to be known to compile it

                Ex: C and Pascal

        2.2.2: Tree-walk Interpreters
                Some languages begin executing code right after parsing it to an
                AST

                To run the program, the interpreter traversees the syntax tree one
                branch and leaf at a time, evaluating each node as it goes

                Really slow

        2.2.3: Transpilers
                Targets another source language as if it were an intermediate representation

                Create compiler and then target another source language that is as high level
                as yours. Leverage existing tools for that language to compile.

        2.2.4: Just-in-time Compilation
                Compile code during loading of application

2.3: Compilers and Interpreters
        Compiling - an implementation technique that involves translating
        a source language to some other -- usually lower-level -- form.
                Ex: Generating bytecode or machine code
                    Transpiling to another high-level language

        Compiler - Translates source code to some other form but doesn't execute it

        Interpreter - Takes in source code and executes it immediatly.
        Runs program "from source"

        GCC / and Clang are compilers

        Earlier versions of Ruby were interpreters

        CPython internally compiles to bytecode then runs it in a VM.
        So it is both a compiler and an interpreter
